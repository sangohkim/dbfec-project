{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (24.0)\n",
      "Collecting pandas==1.3.1\n",
      "  Using cached pandas-1.3.1.tar.gz (4.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/choeseoyeong/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/choeseoyeong/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/choeseoyeong/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/54/jbt5bfjn4m3_lyj17zjtjdkr0000gn/T/pip-build-env-8jo6jjhb/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 325, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/54/jbt5bfjn4m3_lyj17zjtjdkr0000gn/T/pip-build-env-8jo6jjhb/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 295, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/54/jbt5bfjn4m3_lyj17zjtjdkr0000gn/T/pip-build-env-8jo6jjhb/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 480, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/54/jbt5bfjn4m3_lyj17zjtjdkr0000gn/T/pip-build-env-8jo6jjhb/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 18, in <module>\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'numpy'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Requirement already satisfied: nltk in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (3.6.3)\n",
      "Requirement already satisfied: click in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /Users/choeseoyeong/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choeseoyeong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/choeseoyeong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/choeseoyeong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/choeseoyeong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.020962619096248056\n",
      "std: 0.034120015174726695\n",
      "mean: 0.025444851441606575\n",
      "std: 0.03536433413810733\n",
      "mean: 0.020917579448668646\n",
      "std: 0.03328471680449449\n",
      "mean: 0.02328286885613686\n",
      "std: 0.03491067578404711\n",
      "mean: 0.027071432095394313\n",
      "std: 0.036089852429956215\n",
      "mean: 0.01950747905125426\n",
      "std: 0.03183680831186101\n",
      "mean: 0.025064974905925644\n",
      "std: 0.03549532835412434\n",
      "mean: 0.02206040505873327\n",
      "std: 0.03463747154911742\n",
      "mean: 0.023232691580728815\n",
      "std: 0.036029786920107026\n",
      "mean: 0.02639829604939488\n",
      "std: 0.03594011316974839\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pandas==1.3.1\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numba\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from IPython.display import Javascript\n",
    "# json 파일 업로드 및 읽기\n",
    "# file_path_list = [\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2015-1-1-2016-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2016-1-1-2017-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2017-1-1-2018-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2018-1-1-2019-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2019-1-1-2020-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2020-1-1-2021-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2021-1-1-2022-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2022-1-1-2023-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter-onlydata-2023-1-1-2024-1-1.json\",\"/Users/choeseoyeong/Downloads/kickstarter_filtered_total.json\" ]\n",
    "def process_json_files(input_folder_path, output_folder_path):\n",
    "    # 입력 폴더 내의 모든 JSON 파일을 처리\n",
    "    for file_name in os.listdir(input_folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            input_file_path = os.path.join(input_folder_path, file_name)\n",
    "            output_file_path = os.path.join(output_folder_path, file_name.replace('.json', '_ESG_scores.json'))\n",
    "            process_json_file(input_file_path, output_file_path)\n",
    "\n",
    "def process_json_file(input_file_path, output_file_path):\n",
    "    data_array = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "            try:\n",
    "                data_array = json.load(file)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                print(input_file_path)\n",
    "    # 데이터를 저장할 리스트\n",
    "    dataset = []\n",
    "\n",
    "    # 데이터에서 ['name', 'blurb', 'slug', 'creator name'] 형태의 데이터 리스트 추출\n",
    "    for data in data_array:\n",
    "        datatotal = []\n",
    "        if 'name' in data:\n",
    "            datatotal.append(data['name'].replace(\"'\", \"\"))\n",
    "        if 'blurb' in data:\n",
    "            datatotal.append(data['blurb'].replace(\"'\", \"\"))\n",
    "        if 'slug' in data:\n",
    "            datatotal.append(data['slug'].replace(\"'\", \"\"))\n",
    "        if 'creator' in data and 'name' in data['creator']:\n",
    "            datatotal.append(data['creator']['name'].replace(\"'\", \"\"))\n",
    "        dataset.append(datatotal)\n",
    "    list1 = ['air', 'biofertilizer', 'biogas', 'carbon', 'climate', 'conservation', 'conserve', 'conserved', 'conserves', 'conserving', 'contaminate', 'eco-activities', 'eco-friendly', 'ecology', 'emission', 'emit', 'energy', 'energy-efficient', 'environment', 'erode', 'erosion', 'externality', 'fertilis', 'fertiliz', 'greenlife', 'landscaping', 'natural', 'pollute', 'preserve', 'purify', 'recharge', 're-charge', 'recycle', 'salvage', 'solar', 'sustainability', 'sustainable', 'toxic', 'unpollute', 'unspoil', 'waste', 'water']\n",
    "    list2 = ['clean', 'environmental', 'epa', 'sustainability', 'climate', 'warming', 'biofuel', 'biofuels', 'green', 'renewable', 'solar', 'stewardship', 'wind', 'atmosphere', 'emission', 'emissions', 'emit', 'ghg', 'ghgs', 'greenhouse', 'agriculture', 'deforestation', 'pesticide', 'pesticides', 'wetlands', 'zoning', 'biodiversity', 'species', 'wilderness', 'wildlife', 'freshwater', 'groundwater', 'water', 'cleaner', 'cleanup', 'coal', 'contamination', 'fossil', 'resource', 'air', 'carbon', 'nitrogen', 'pollution', 'superfund', 'biphenyls', 'hazardous', 'householding', 'pollutants', 'printing', 'recycle', 'recycling', 'toxic', 'waste', 'wastes', 'weee', 'climate change', 'conservation', 'environmentally', 'footprint', 'global warming', 'pollutant', 'recycled', 'sustainable', 'sustainably']\n",
    "    environment_dictionary = list(set(list1 + list2))\n",
    "    list11 = ['accountable', 'benefice', 'beneficiar', 'beneficial', 'communit', 'compassion', 'concerned', 'cooperate', 'development', 'educate', 'benefit', 'benevolen', 'brotherhood', 'care', 'caring', 'charit', 'civic', 'class', 'classes', 'concern', 'cultivating', 'empower', 'equal', 'equality', 'familial', 'families', 'family', 'freedom', 'graduation', 'happiness', 'happy', 'harmony', 'harvesting', 'harvests', 'health', 'help', 'humanity', 'humankind', 'immuniz', 'independent', 'joy', 'justice', 'kind', 'learn', 'liberate', 'liberty', 'life', 'mankind', 'partnership', 'peace', 'prosper', 'reading', 'responsibilities', 'rights', 'social', 'societ', 'SROI', 'success', 'support', 'teach', 'tender', 'trustworth', 'virtu', 'welfare', 'wellbeing', 'well-being', 'wisdom']\n",
    "    list22 = ['citizen','citizens', 'csr','disabilities', 'disability', 'disabled', 'human', 'nations', 'social', 'un', 'veteran', 'veterans', 'vulnerable', 'dignity', 'discriminate', 'discriminated', 'discriminating', 'discrimination', 'equality', 'freedom', 'humanity', 'nondiscrimination', 'sexual', 'communities', 'community', 'expression', 'marriage', 'privacy', 'peace', 'bargaining', 'eeo', 'fairness', 'fla', 'harassment', 'injury', 'labor', 'overtime', 'ruggie', 'sick', 'wage', 'wages', 'workplace', 'bisexual', 'diversity', 'ethnic', 'ethnically', 'ethnicities', 'ethnicity', 'female', 'females', 'gay', 'gays', 'gender', 'genders', 'homosexual', 'immigration', 'lesbian', 'lesbians', 'lgbt', 'minorities', 'minority', 'ms', 'race', 'racial', 'religion', 'religious', 'sex', 'transgender', 'woman', 'women', 'occupational', 'safe', 'safely', 'safety', 'ilo', 'labour', 'eicc', 'children', 'epidemic', 'health', 'healthy', 'ill', 'illness', 'pandemic', 'childbirth', 'drug', 'medicaid', 'medicare', 'medicine', 'medicines', 'hiv', 'alcohol', 'drinking', 'bugs', 'conformance', 'defects', 'fda', 'inspection', 'inspections', 'minerals', 'standardization', 'warranty', 'endowment', 'endowments', 'people', 'philanthropic', 'philanthropy', 'socially', 'societal', 'society', 'welfare', 'charitable', 'charities', 'charity', 'donate', 'donated', 'donates', 'donating', 'donation', 'donations', 'donors', 'foundation', 'foundations', 'gift', 'gifts', 'nonprofit', 'poverty', 'courses', 'educate', 'educated', 'educates', 'educating', 'education', 'educational', 'learning', 'mentoring', 'scholarships', 'teach', 'teacher', 'teachers', 'teaching', 'training', 'employ', 'employment', 'headcount', 'hire', 'hired', 'hires', 'hiring', 'staffing', 'unemployment']\n",
    "    social_dictionary = list(set(list11 + list22))\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    def remove_stopwords(words):\n",
    "        # 불용어 및 특수문자 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
    "        return filtered_words\n",
    "    # environment_dictionay와 social_dictionary에 불용어 제거만 수행\n",
    "    environment_dictionary = remove_stopwords(environment_dictionary)\n",
    "    social_dictionary = remove_stopwords(social_dictionary)\n",
    "    # NLTK의 word_tokenize를 통해 단어 토큰화 수행 함수\n",
    "    def tokenize(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # 불용어 및 특수문자 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    # LDA에 포함할 리스트\n",
    "    topiclist = []\n",
    "    # dataset = [['Cozy Kornerz: Doing the Dezigning ☁️', \"Cozy Kornerz is a fun new redecorating tool that'll help bring jaw dropping transformations to your space with ease!\", 'cozy-kornerz-doing-the-dezigning', 'Anna-Marie Haynes'], ['Full Body Exercise Matt Double Sided', 'I want to design an exercise Matt with practice instructions that will increase your performance through out the day.', 'full-body-exercise-matt-double-sided', 'Milad Nazar'], ['creator name', 'blurb', 'name', 'slug']]\n",
    "    for data in dataset:\n",
    "        data = [tokenize(word) for word in data]\n",
    "        # 빈 요소 제거\n",
    "        data = [v for v in data if v]\n",
    "        from gensim import corpora\n",
    "        dictionary = corpora.Dictionary(data)\n",
    "        corpus = [dictionary.doc2bow(text) for text in data]\n",
    "        import gensim\n",
    "        NUM_TOPICS = 1\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=9999999)\n",
    "        for topic in topics:\n",
    "            topiclist.append(topic)\n",
    "    # print(topiclist)\n",
    "    # print(len(topiclist))\n",
    "    # print(dataset)\n",
    "    # ESG factor 계산\n",
    "    from scipy.special import softmax\n",
    "    # weight*\"word\" 형태를 weight과 word로 분리\n",
    "    def split_weight_and_word(s):\n",
    "        parts = s.split('\"')\n",
    "        weight = float(parts[0].strip('*').strip())\n",
    "        word = parts[1]\n",
    "        return weight, word\n",
    "\n",
    "    def calculate_scores(l1, l2):\n",
    "        scores = []\n",
    "\n",
    "        for tuple_item in l1:\n",
    "            # 튜플에서 weight과 string 추출\n",
    "            weight, string = tuple_item\n",
    "\n",
    "            # string을 토큰화\n",
    "            sum = 0\n",
    "            worddlist = []\n",
    "            weightlist = []\n",
    "            words = [word.strip('\\\"*') for word in string.split('+')]\n",
    "            for weightandword in words:\n",
    "                weight, word = split_weight_and_word(weightandword)\n",
    "                weightlist.append(weight)\n",
    "                worddlist.append(word)\n",
    "            # softmax 함수 적용\n",
    "            softmax_weightlist = softmax(weightlist)\n",
    "            for i in range(len(worddlist)):\n",
    "                if worddlist[i] in l2:\n",
    "                    sum+=softmax_weightlist[i]\n",
    "\n",
    "            # 총 score 계산\n",
    "            scores.append(sum)\n",
    "\n",
    "\n",
    "        return scores\n",
    "\n",
    "    # 점수 계산 및 출력\n",
    "    import math\n",
    "    ESG_scores_list = []\n",
    "    # ENV_list = calculate_scores(textstopic, environment_dictionary)\n",
    "    # S_list = calculate_scores(textstopic, social_dictionary)\n",
    "    ENV_list = calculate_scores(topiclist, environment_dictionary)\n",
    "    S_list = calculate_scores(topiclist, social_dictionary)\n",
    "    for i in range(len(ENV_list)):\n",
    "        ENV_scores = ENV_list[i]\n",
    "        S_scores = S_list[i]\n",
    "        data_array[i]['ENV_scores'] = ENV_scores\n",
    "        data_array[i]['S_scores'] = S_scores\n",
    "        if ENV_scores == 0 and S_scores == 0:\n",
    "            ESG_scores = 0\n",
    "        else:\n",
    "            ESG_scores = math.sqrt((ENV_scores**2 + S_scores**2)/2)\n",
    "            # ESG_scores = (ENV_scores + S_scores)/2\n",
    "            # ESG_scores = (ENV_scores + S_scores)/2\n",
    "            data_array[i]['ESG_scores']=ESG_scores\n",
    "        ESG_scores_list.append(ESG_scores)\n",
    "    # print(len(ESG_scores_list))\n",
    "    # ESG Score 평균 및 표준편차 계산\n",
    "    import numpy\n",
    "    print(\"mean:\",numpy.mean(ESG_scores_list))\n",
    "    print(\"std:\", numpy.std(ESG_scores_list))\n",
    "    with open(output_file_path, 'w') as new_file:\n",
    "        json.dump(data_array, new_file, indent=4)\n",
    "input_folder_path = \"/Users/choeseoyeong/Desktop/raw_data\"\n",
    "output_folder_path = \"/Users/choeseoyeong/Desktop/new_data\"\n",
    "process_json_files(input_folder_path, output_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
